\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{boondox-cal}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{svg}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning, arrows.meta}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Problem Set\ \#1}
\newcommand{\hmwkDueDate}{February 28, 2024}
\newcommand{\hmwkClass}{CS229}
\newcommand{\hmwkClassInstructor}{Andrew Ng}
\newcommand{\hmwkAuthorName}{\textbf{0130}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% empty underline
\newcommand{\emptyunderline}{\underline{\ \ \ \ \ \ }}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item \[
                \begin{aligned}
                \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} -\frac{1}{m} \sum_{i=1}^m y^{(i)} \log (h_{\theta}(x^{(i)})) + (1 -
                y^{(i)} \log (1 - h_{\theta}(x^{(i)}))) \\
                &= - \frac{1}{m} \sum_{i=1}^m [ y^{(i)} \frac{1}{h_{\theta}(x^{(i)})}  - (1 - y^{(i)}) \frac{1}{1-h_{\theta}(x^{(i)})} ] \frac{\partial}{\partial \theta_j} h_{\theta}(x^{(i)})
                \end{aligned}
            \]
            Then we calculate:
            \[ 
                \frac{\partial}{\partial\theta_j} h_{\theta}(x^{(i)}) = \frac{\partial}{\partial \theta_j} g(\theta^T x^{(i)}) = g(\theta^T x^{(i)})(1-g(\theta^T x^{(i)})) x_j^{(i)} = h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) x_j^{(i)}
            \]
            And we can further simplify the above equation:
            \[
                \begin{aligned}
                \frac{\partial}{\partial \theta_j} J(\theta) &= -\frac{1}{m} \sum_{i=1}^m [(y^{(i)} - h_\theta(x^{(i)})) - (1-y^{(i)})h_{\theta}(x^{(i)})] x_{j}^{(i)} \\
                &= -\frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \\
                \end{aligned}
            \]

            Then we can get second-order derivative:

            \[
                \begin{aligned}
                    H_{ij} = \frac{\partial^2}{\partial \theta_i \partial \theta_j} J(\theta)  &= \frac{1}{m} \sum_{k=1}^m x_j \frac{\partial}{\partial \theta_i} h_{\theta}(x^{(k)}) \\
                    &= \frac{1}{m} \sum_{k=1}^m x_{i}^{(k)} x_{j}^{(k)} h_{\theta}(x^{(k)})(1-h_{\theta}(x^{(k)}))\\
                \end{aligned}
            \]
            for each vector z, consider the quadratic form of Hessian matrix:

            \[
                \begin{aligned}
                    z^T H z &= \sum_{i=1}^n \sum_{j=1}^n z_i H_{ij} z_j = \frac{1}{m} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^m z_i x_i^{(k)} z_j x_j^{(k)}h_\theta(x^{(k)})(1-h_{\theta}(x^{(k)})) \\
                    &= \frac{1}{m} \sum_{k=1}^m (\sum_{i=1}^n \sum_{j=1}^n z_i x_i^{(k)} z_j x_{j}^{(k)}) h_{\theta}(x^{(k)}) (1-h_\theta(x^{(k)})) \\
                    &= \frac{1}{m}\sum_{k=1}^m {(x^{(k)T}z)}^2 h_{\theta}(x^{(k)}) (1 - h_\theta(x^{(k)})) \geq 0 \Leftrightarrow H \succeq 0
                \end{aligned}
            \]
        \item Codes are shown in src director, see \texttt{src/p01b\_logreg.py}.
        \item 
            \[
                \begin{aligned}
                    P(y=1 | x ; \phi, \mu_0, \mu_1, \Sigma) &= \frac{P(x|y; \phi, \mu_0, \mu_1, \Sigma)}{P(x|y=1; \phi, \mu_0, \mu_1, \Sigma)P(y=1) + P(x|y=0; \phi, \mu_0, \mu_1, \Sigma)P(y=0)} \\
                    &= \frac{\exp (-1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1) )}{\exp (-1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1) )\phi + \exp (-1/2{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0) )(1-\phi)} \\
                    &= \frac{1}{1 + ((1-\phi)/\phi) \exp (-1/2{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0) + 1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1))}
                \end{aligned}
            \]
        then we calculate the portion of the denominator inside the exponential term using the fact that: (1) 
        \( \Sigma \) is symmetric (2) \( \text{if } A \text{ is symmetric, then } {(A^{-1})}^T = {(A^T)}^{-1}. \)
            \[
                \begin{aligned}
                1/2 ({(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1)-{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0)) \\
                 = {(\mu_0 - \mu_1)}^T \Sigma^{-1} x + 1/2(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T \Sigma^{-1}\mu_1)
                \end{aligned}
            \]
        and this allow us to simplify the first equation and prove that the decision boundary is linear:
        \[
            \begin{aligned}
                P(y=1|x;\phi, \mu_0, \mu_1, \Sigma) &= \frac{1}{1 + \exp(-(\theta^T x + \theta_0))} \\
                \theta &= \Sigma^{-1} (\mu_0 - \mu_1)   \\
                \theta_0 &= 1/2(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T\Sigma^{-1}\mu_1) + \log(1-\phi) - \log\phi
            \end{aligned}
        \]
    \end{enumerate}
\end{homeworkProblem}


\begin{homeworkProblem}
    
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item \[
            \begin{aligned}
            p(y; \lambda) &= \frac{e^{-\lambda} \lambda^y}{y!} = \exp(\log(\frac{e^{-\lambda} \lambda^y}{y!})) \\
            &= \exp (-\lambda + y\log \lambda - \log y!) = \frac{1}{y!}(\log \lambda y - \lambda) \\
            &\Rightarrow b(y) = \frac{1}{y!},\ \eta = \log \lambda,\ T(y) = y,\ \alpha(\eta) = \lambda = e^{\eta}
            \end{aligned}
        \]
        \item Let canonical response function represented as \( g \), then we have:
        \[
            \lambda = g(\eta) = e^\eta
        \]
        then we know \( g = \exp \).
        \item \[
            \begin{aligned}
                \log p(y | \lambda) &= -\lambda + y\log \lambda - \log y! \Rightarrow \log p(y^{(i)} | x^{(i)}; \theta) \\
                &= -e^{\theta^T x} + y^{(i)} \theta^T x^{(i)} - \log y^{(i)}!
            \end{aligned}
        \]
        and we can calculate the derivative of \( \log p(y^{(i)} | x^{(i)} ; \theta) \) with respect to \( \theta_j \):
        \[
            \frac{\partial}{\partial \theta_j} \log p(y^{(i)} | x^{(i)} ; \theta) = -e^{(\theta^T x)}x_j^{(i)} + y^{(i)}x_j^{(i)}
        \]
        we then get the gradient ascent update rules as follows:
        \[
            \theta_j := \theta + \alpha \frac{\partial}{\partial \theta_j} \log p(y^{(i)} | x^{(i)}; \theta) = \theta_j + \alpha(y^{(i)} - e^{\theta^T x}) x_j^{(i)}
        \]
        In fact, the member in GLM has similar stochastic gradient ascent update rules:
        \[
            \begin{aligned}
                \theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)} = \theta_j + \alpha(y^{(i)} - \mathbb{E}(y^{(i)}|x^{(i)}; \theta))x_j^{(i)} \\
            \end{aligned}
        \]
        since \( \mathbb{E}(y | x; \theta) = \exp{(\theta^T x)} \), we get the same answer.
        \item Codes are shown in src directory, see \texttt{src/p03d\_poisson.py}.
    \end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item 
        From the property of the probability space we get:
        \[
            \begin{aligned}
                \int_{\Omega} p(y;\eta) dy = 1 &= \frac{1}{\exp(\alpha(\eta))} \int_{\Omega} b(y) \exp(\eta y) dy
            \end{aligned}
        \]
        which is equivalent to:
        \[
            \exp(\alpha(\eta)) = \int_{\Omega}b(y)\exp(\eta y)dy
        \]
        apply the partial derivatives to both sides, we have:
        \[
            \frac{\partial}{\partial \eta} \exp(\alpha(\eta)) = \exp(\alpha(\eta)) \frac{\partial}{\partial \eta} \alpha(\eta) = \frac{\partial}{\partial \eta} \int_{\Omega} b(y) \exp(\eta y) dy = \int_{\Omega} b(y) \exp(\eta y) y dy
        \]
        after some algebraic manipulation, we get:
        \[
            \frac{\partial}{\partial \eta} \alpha(\eta) = \int_{\Omega} b(y) y \exp(\eta y - \alpha(\eta)) dy = \mathbb{E}[Y | X; \theta]
        \]
        \item
        \[
            \begin{aligned}
                \frac{\partial^2}{\partial \eta^2} \alpha(\eta) &= \frac{\partial}{\partial \eta} (\frac{\partial}{\partial \eta} \alpha(\eta)) = \frac{1}{\exp(\alpha(\eta))} \int_{\Omega} b(y) y^2 \exp(\eta y) dy - \frac{1}{\exp(\alpha(\eta))} \frac{\partial}{\partial \eta}\alpha(\eta)\int_\Omega b(y) y \exp(\eta y) dy \\
                \newline
                &= \mathbb{E}[Y^2 | X; \theta] - \mathbb{E}{[Y | X; \theta]}^2 = \Var[Y | X; \theta]
            \end{aligned}
        \]
        \item We can formulate the loss function as follows:
        \[
            l(\theta) = -\log J(\theta) = -\log P(Y|X;\theta)
        \]
        where \( J(\theta) \) is the likelihood function. In order to get the hessian of the loss function, we first calculate the first-order derivative of the loss function:
        \[
            \frac{\partial}{\partial \theta_j} l(\theta) = \frac{-1}{p(y;\eta)} \frac{\partial}{\partial \eta} p(y;\eta) \frac{\partial}{\partial \theta_j} \eta = \frac{-x_j}{p(y;\eta)} \frac{\partial}{\partial \eta} p(y;\eta)
        \]
        then we calculate the second-order derivative of the loss function:
        \[
            \begin{aligned}
            \frac{\partial^2}{\partial \theta_i \partial \theta_j} l(\theta) &= \frac{x_j}{{p(y;\eta)}^2} \frac{\partial}{\partial \eta} p(y;\eta) \frac{\partial}{\partial \theta_i} \eta \frac{\partial}{\partial \eta} p(y;\eta) - \frac{x_j}{p(y;\eta)} \frac{\partial^2}{\partial \eta^2}p(y;\eta) \frac{\partial}{\partial \theta_i} \eta \\
            &= \frac{x_i x_j}{p{(y;\eta)}^2} {(\frac{\partial}{\partial \eta} p(y;\eta))}^2 - \frac{x_i x_j}{p(y;\eta)} \frac{\partial^2}{\partial \eta^2} p(y; \eta)
            \end{aligned}
        \]
        by calculating the first-order and second-order derivatives of \( p(y;\eta) \), we have:
        \[
            \begin{aligned}
                \frac{\partial}{\partial \eta} p(y;\eta) &= b(y)\exp(\eta y - \alpha(\eta)) (y - \frac{\partial}{\partial \eta}\alpha(\eta)) = p(y; \eta) (y - \frac{\partial}{\partial \eta}\alpha(\eta)) \\
                \frac{\partial^2}{\partial \eta^2} p(y;\eta) &= p(y;\eta){(y-\frac{\partial}{\partial \eta}\alpha(\eta))}^2 - p(y;\eta) \frac{\partial^2}{\partial \eta^2} \alpha(\eta)
            \end{aligned}
        \]
        then we can further simplify the second-order derivative of the loss function by some algebraic manipulation, and the result is:
        \[
            \frac{\partial^2}{\partial \theta_i \theta_j} l(\theta) = x_i x_j \Var[Y|X;\theta]
        \]
        consider the quadratic form of the hessian matrix, we have:
        \[
            z^T H z = \Var[Y|X; \theta]\sum_i \sum_j x_i x_j z_i z_j = \Var[Y|X;\theta] (z^T x) \geq 0, \forall z \in \mathbb{R}^n
        \]
        
    \end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    
\end{homeworkProblem}

\end{document}