\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{boondox-cal}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{tikz}
\usepackage{svg}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}

\usetikzlibrary{automata,positioning, arrows.meta}

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkAuthorName}
\chead{\hmwkClass\ (\hmwkClassInstructor): \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \arabic{#1} (continued)}{Problem \arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[1][-1]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \arabic{homeworkProblemCounter}}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%

\newcommand{\hmwkTitle}{Problem Set\ \#1}
\newcommand{\hmwkDueDate}{February 28, 2024}
\newcommand{\hmwkClass}{CS229}
\newcommand{\hmwkClassInstructor}{Andrew Ng}
\newcommand{\hmwkAuthorName}{\textbf{0130}}

%
% Title Page
%

\title{
    \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
    \vspace{0.1in}\large{\textit{\hmwkClassInstructor}}
    \vspace{3in}
}

\author{\hmwkAuthorName}
\date{}

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

% empty underline
\newcommand{\emptyunderline}{\underline{\ \ \ \ \ \ }}

\begin{document}

\maketitle

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item \[
                \begin{aligned}
                \frac{\partial}{\partial \theta_j} J(\theta) &= \frac{\partial}{\partial \theta_j} -\frac{1}{m} \sum_{i=1}^m y^{(i)} \log (h_{\theta}(x^{(i)})) + (1 -
                y^{(i)} \log (1 - h_{\theta}(x^{(i)}))) \\
                &= - \frac{1}{m} \sum_{i=1}^m [ y^{(i)} \frac{1}{h_{\theta}(x^{(i)})}  - (1 - y^{(i)}) \frac{1}{1-h_{\theta}(x^{(i)})} ] \frac{\partial}{\partial \theta_j} h_{\theta}(x^{(i)})
                \end{aligned}
            \]
            Then we calculate:
            \[ 
                \frac{\partial}{\partial\theta_j} h_{\theta}(x^{(i)}) = \frac{\partial}{\partial \theta_j} g(\theta^T x^{(i)}) = g(\theta^T x^{(i)})(1-g(\theta^T x^{(i)})) x_j^{(i)} = h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) x_j^{(i)}
            \]
            And we can further simplify the above equation:
            \[
                \begin{aligned}
                \frac{\partial}{\partial \theta_j} J(\theta) &= -\frac{1}{m} \sum_{i=1}^m [(y^{(i)} - h_\theta(x^{(i)})) - (1-y^{(i)})h_{\theta}(x^{(i)})] x_{j}^{(i)} \\
                &= -\frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j^{(i)} \\
                \end{aligned}
            \]

            Then we can get second-order derivative:

            \[
                \begin{aligned}
                    H_{ij} = \frac{\partial^2}{\partial \theta_i \partial \theta_j} J(\theta)  &= \frac{1}{m} \sum_{k=1}^m x_j \frac{\partial}{\partial \theta_i} h_{\theta}(x^{(k)}) \\
                    &= \frac{1}{m} \sum_{k=1}^m x_{i}^{(k)} x_{j}^{(k)} h_{\theta}(x^{(k)})(1-h_{\theta}(x^{(k)}))\\
                \end{aligned}
            \]
            for each vector z, consider the quadratic form of Hessian matrix:

            \[
                \begin{aligned}
                    z^T H z &= \sum_{i=1}^n \sum_{j=1}^n z_i H_{ij} z_j = \frac{1}{m} \sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^m z_i x_i^{(k)} z_j x_j^{(k)}h_\theta(x^{(k)})(1-h_{\theta}(x^{(k)})) \\
                    &= \frac{1}{m} \sum_{k=1}^m (\sum_{i=1}^n \sum_{j=1}^n z_i x_i^{(k)} z_j x_{j}^{(k)}) h_{\theta}(x^{(k)}) (1-h_\theta(x^{(k)})) \\
                    &= \frac{1}{m}\sum_{k=1}^m {(x^{(k)T}z)}^2 h_{\theta}(x^{(k)}) (1 - h_\theta(x^{(k)})) \geq 0 \Leftrightarrow H \succeq 0
                \end{aligned}
            \]
        \item Codes are shown in src director, see \texttt{src/p01b\_logreg.py}.
        \item 
            \[
                \begin{aligned}
                    P(y=1 | x ; \phi, \mu_0, \mu_1, \Sigma) &= \frac{P(x|y; \phi, \mu_0, \mu_1, \Sigma)}{P(x|y=1; \phi, \mu_0, \mu_1, \Sigma)P(y=1) + P(x|y=0; \phi, \mu_0, \mu_1, \Sigma)P(y=0)} \\
                    &= \frac{\exp (-1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1) )}{\exp (-1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1) )\phi + \exp (-1/2{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0) )(1-\phi)} \\
                    &= \frac{1}{1 + ((1-\phi)/\phi) \exp (-1/2{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0) + 1/2{(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1))}
                \end{aligned}
            \]
        then we calculate the portion of the denominator inside the exponential term using the fact that: (1) 
        \( \Sigma \) is symmetric (2) \( \text{if } A \text{ is symmetric, then } {(A^{-1})}^T = {(A^T)}^{-1}. \)
            \[
                \begin{aligned}
                1/2 ({(x-\mu_1)}^T \Sigma^{-1} (x-\mu_1)-{(x-\mu_0)}^T \Sigma^{-1} (x-\mu_0)) \\
                 = {(\mu_0 - \mu_1)}^T \Sigma^{-1} x + 1/2(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T \Sigma^{-1}\mu_1)
                \end{aligned}
            \]
        and this allow us to simplify the first equation and prove that the decision boundary is linear:
        \[
            \begin{aligned}
                P(y=1|x;\phi, \mu_0, \mu_1, \Sigma) &= \frac{1}{1 + \exp(-(\theta^T x + \theta_0))} \\
                \theta &= \Sigma^{-1} (\mu_1 - \mu_0)   \\
                \theta_0 &= 1/2(\mu_0^T\Sigma^{-1}\mu_0 - \mu_1^T\Sigma^{-1}\mu_1) + \log(1-\phi) - \log\phi
            \end{aligned}
        \]
    \item Firstly, simplify the formula of \( l \):
        \[
            l(\phi, \mu_0, \mu_1, \Sigma) = \log \prod_{i=1}^m p(x^{(i)}, y^{(i)}, \phi, \mu_0, \mu_1, \Sigma) \\
        \]
        \[
            = -\frac{mn}{2} \log(2\pi) -\frac{m}{2} \log(|\Sigma|) - \frac{1}{2}\sum_{i=1}^m{(x^{(i)} - \mu_{y^{(i)}})}^T\Sigma^{-1}{(x^{(i)} - \mu_{y^{(i)}})} + \sum_{i=1}^m y^{(i)}\log(\phi) + (1-y^{(i)}) \log(1-\phi)
        \]
        let \( \frac{\partial}{\partial \phi} l = 0 \), we have
        \[
            \frac{\partial}{\partial \phi} l = \sum_{i=1}^m \frac{(1-\phi)y^{(i)} - \phi(1-y^{(i)})}{\phi(1-\phi)} = 0 \Rightarrow \phi = \frac{1}{m}\sum_{i=1}^m 1\{ y^{(i)} = 1 \}
        \]
        let \( \frac{\partial}{\partial \mu_{y^{(i)}}} l = 0 \), we have
        \[
            \begin{cases}
            \frac{\partial}{\partial \mu_{y^{(i)}}} l &= \frac{1}{2} \sum_{i=1}^m 2 \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) = \Sigma^{-1} \sum_{i=1}^m (x^{(i)} - \mu_{y^{(i)}}) = 0 \\
            \frac{\partial}{\partial \mu_0} \mu_{y^{(i)}} &= 1\{ y^{(i)} = 0 \}, \frac{\partial}{\partial \mu_1}\mu_{y^{(i)}} = 1\{ y^{(i)} = 1\}
            \end{cases}
        \]
        using the chain rule, we get:
        \[
            \frac{\partial}{\partial \mu_k} l = \frac{\partial}{\partial \mu_{y^{(i)}}} l \frac{\partial}{\partial \mu_k} \mu_{y^{(i)}} = 0 \Rightarrow \mu_k = \frac{\sum_{i=1}^m 1\{ y^{(i)} = k \} x^{(i)}}{\sum_{i=1}^m 1\{ y^{(i)} = k \}}, k = 0,1
        \]
        we don't need to assume \( n = 1 \), let's consider a more general case, let \( \frac{\partial}{\partial \Sigma} l = 0 \):
        \[
            \frac{\partial}{\partial \Sigma} l = \frac{\partial}{\partial \Sigma} \{ -\frac{m}{2}\log(|\Sigma|) - \frac{1}{2} \sum_{i=1}^m {(x^{(i)} - \mu_{y^{(i)}})}^T \Sigma^{-1} (x^{(i)} - \mu_{y^{(i)}}) \} = 0
        \]
        let's first calculate the derivative of the determinant, suppose \( D = (\delta_1, \delta_2, \ldots, \delta_n) \) and 
        
        \( E = (\epsilon_1, \epsilon_2, \ldots, \epsilon_n) \), we have:
        \[
            \det(D+tE) - \det(D) = \sum_{i=1}^n \det(\delta_1, \delta_2,\ldots, t\epsilon_i, \ldots, \delta_n)
        \]
        let \( \epsilon_i = \sum_{j} f_{ij} \delta_j \) which is equivalent to \( E = DF \) where \( F_{ij} = f_{ij} \), then the above equation can be written as:
        \[
            \sum_{i=1}^n \det(\delta_1, \delta_2, \ldots, t\epsilon_i, \ldots, \delta_n) = \sum_{i=1}^n tf_{ii} \det(D) = \text{tr}(D^{-1}E) \cdot t \det(D)
        \]
        then we know the derivative of the determinant is:
        \[
            \frac{\partial}{\partial \Sigma} \det(\Sigma) = \det(\Sigma) \text{tr}(\Sigma^{-1}E)
        \]
        then we can calculate the derivative of the log determinant:
        \[
            \frac{\partial}{\partial \Sigma} \log(\det(\Sigma)) = \frac{1}{\det(\Sigma)} \det(\Sigma) \text{tr}(\Sigma^{-1}E) = \text{tr}(\Sigma^{-1}E)
        \]
        next, we calculate the derivative of the inverse of the matrix:
        \[
            \begin{aligned}
            {(D + tE)}^{-1} - D^{-1} &= {(D(I+tD^{-1}E))}^{-1} - D^{-1} \\
            &= (I-tD^{-1}E + O(t^2))D^{-1} - D^{-1} \\
            &= -tD^{-1}ED^{-1} + O(t^2)
            \end{aligned}
        \]
        then we know the derivative of the inverse of the matrix is:
        \[
            \frac{\partial}{\partial \Sigma} \Sigma^{-1} = -\Sigma^{-1} E \Sigma^{-1}
        \]
        using the fact that \( \frac{\partial}{\partial A} AB = B^T \), let \( u^{(i)} = (x^{(i)} - \mu_{y^{(i)}})\), we know the equation of \( \frac{\partial}{\partial \Sigma} l = 0 \) is equivalent to:
        \[
            \begin{aligned}
                &-\frac{m}{2} \text{tr}(\Sigma^{-1}E) + \frac{1}{2}\sum_{i=1}^m {u^{(i)}}^T \Sigma^{-1} E \Sigma^{-1} {u^{(i)}} = 0  \\
            \Leftrightarrow & \text{tr} (\sum_{i=1}^m {u^{(i)}}^T \Sigma^{-1} E \Sigma^{-1} u^{(i)} - m\Sigma^{-1}E) = 0    \\
            \Leftrightarrow & \text{tr} ((\sum_{i=1}^m \Sigma^{-1} u^{(i)} {u^{(i)}}^T - mI)\Sigma^{-1} E) = 0, \forall E   \\
            \Leftrightarrow & \sum_{i=1}^m \Sigma^{-1} u^{(i)} {u^{(i)}}^T = mI \\
            \Leftrightarrow & \Sigma = \frac{1}{m} \sum_{i=1}^m (x^{(i)} - \mu_{y^{(i)}}) {(x^{(i)} - \mu_{y^{(i)}})}^T
            \end{aligned}
        \]
        \item Codes are shown in src director, see \texttt{src/p01e\_gda.py}.
        \item Figure is shown in \texttt{src/output/p01f\_plot.png}. Logistic regression is better than GDA in this case, since the
        \( p(x | y) \) may not be Gaussian.
        \item Figure is shown in \texttt{src/output/p01g\_plot.png}.
        \item The idea of this problem is that we can choose a transformation of \( x \) such that \( p(x|y) \) is Gaussian. (See Box-Cox transformation)
    \end{enumerate}
\end{homeworkProblem}


\begin{homeworkProblem}


    
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item \[
            \begin{aligned}
            p(y; \lambda) &= \frac{e^{-\lambda} \lambda^y}{y!} = \exp(\log(\frac{e^{-\lambda} \lambda^y}{y!})) \\
            &= \exp (-\lambda + y\log \lambda - \log y!) = \frac{1}{y!}(\log \lambda y - \lambda) \\
            &\Rightarrow b(y) = \frac{1}{y!},\ \eta = \log \lambda,\ T(y) = y,\ \alpha(\eta) = \lambda = e^{\eta}
            \end{aligned}
        \]
        \item Let canonical response function represented as \( g \), then we have:
        \[
            \lambda = g(\eta) = e^\eta
        \]
        then we know \( g = \exp \).
        \item \[
            \begin{aligned}
                \log p(y | \lambda) &= -\lambda + y\log \lambda - \log y! \Rightarrow \log p(y^{(i)} | x^{(i)}; \theta) \\
                &= -e^{\theta^T x} + y^{(i)} \theta^T x^{(i)} - \log y^{(i)}!
            \end{aligned}
        \]
        and we can calculate the derivative of \( \log p(y^{(i)} | x^{(i)} ; \theta) \) with respect to \( \theta_j \):
        \[
            \frac{\partial}{\partial \theta_j} \log p(y^{(i)} | x^{(i)} ; \theta) = -e^{(\theta^T x)}x_j^{(i)} + y^{(i)}x_j^{(i)}
        \]
        we then get the gradient ascent update rules as follows:
        \[
            \theta_j := \theta + \alpha \frac{\partial}{\partial \theta_j} \log p(y^{(i)} | x^{(i)}; \theta) = \theta_j + \alpha(y^{(i)} - e^{\theta^T x}) x_j^{(i)}
        \]
        In fact, the member in GLM has similar stochastic gradient ascent update rules:
        \[
            \begin{aligned}
                \theta_j := \theta_j + \alpha(y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)} = \theta_j + \alpha(y^{(i)} - \mathbb{E}(y^{(i)}|x^{(i)}; \theta))x_j^{(i)} \\
            \end{aligned}
        \]
        since \( \mathbb{E}(y | x; \theta) = \exp{(\theta^T x)} \), we get the same answer.
        \item Codes are shown in src directory, see \texttt{src/p03d\_poisson.py}.
    \end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item 
        From the property of the probability space we get:
        \[
            \begin{aligned}
                \int_{\Omega} p(y;\eta) dy = 1 &= \frac{1}{\exp(\alpha(\eta))} \int_{\Omega} b(y) \exp(\eta y) dy
            \end{aligned}
        \]
        which is equivalent to:
        \[
            \exp(\alpha(\eta)) = \int_{\Omega}b(y)\exp(\eta y)dy
        \]
        apply the partial derivatives to both sides, we have:
        \[
            \frac{\partial}{\partial \eta} \exp(\alpha(\eta)) = \exp(\alpha(\eta)) \frac{\partial}{\partial \eta} \alpha(\eta) = \frac{\partial}{\partial \eta} \int_{\Omega} b(y) \exp(\eta y) dy = \int_{\Omega} b(y) \exp(\eta y) y dy
        \]
        after some algebraic manipulation, we get:
        \[
            \frac{\partial}{\partial \eta} \alpha(\eta) = \int_{\Omega} b(y) y \exp(\eta y - \alpha(\eta)) dy = \mathbb{E}[Y | X; \theta]
        \]
        \item
        \[
            \begin{aligned}
                \frac{\partial^2}{\partial \eta^2} \alpha(\eta) &= \frac{\partial}{\partial \eta} (\frac{\partial}{\partial \eta} \alpha(\eta)) = \frac{1}{\exp(\alpha(\eta))} \int_{\Omega} b(y) y^2 \exp(\eta y) dy - \frac{1}{\exp(\alpha(\eta))} \frac{\partial}{\partial \eta}\alpha(\eta)\int_\Omega b(y) y \exp(\eta y) dy \\
                \newline
                &= \mathbb{E}[Y^2 | X; \theta] - \mathbb{E}{[Y | X; \theta]}^2 = \Var[Y | X; \theta]
            \end{aligned}
        \]
        \item We can formulate the loss function as follows:
        \[
            l(\theta) = -\log J(\theta) = -\log P(Y|X;\theta)
        \]
        where \( J(\theta) \) is the likelihood function. In order to get the hessian of the loss function, we first calculate the first-order derivative of the loss function:
        \[
            \frac{\partial}{\partial \theta_j} l(\theta) = \frac{-1}{p(y;\eta)} \frac{\partial}{\partial \eta} p(y;\eta) \frac{\partial}{\partial \theta_j} \eta = \frac{-x_j}{p(y;\eta)} \frac{\partial}{\partial \eta} p(y;\eta)
        \]
        then we calculate the second-order derivative of the loss function:
        \[
            \begin{aligned}
            \frac{\partial^2}{\partial \theta_i \partial \theta_j} l(\theta) &= \frac{x_j}{{p(y;\eta)}^2} \frac{\partial}{\partial \eta} p(y;\eta) \frac{\partial}{\partial \theta_i} \eta \frac{\partial}{\partial \eta} p(y;\eta) - \frac{x_j}{p(y;\eta)} \frac{\partial^2}{\partial \eta^2}p(y;\eta) \frac{\partial}{\partial \theta_i} \eta \\
            &= \frac{x_i x_j}{p{(y;\eta)}^2} {(\frac{\partial}{\partial \eta} p(y;\eta))}^2 - \frac{x_i x_j}{p(y;\eta)} \frac{\partial^2}{\partial \eta^2} p(y; \eta)
            \end{aligned}
        \]
        by calculating the first-order and second-order derivatives of \( p(y;\eta) \), we have:
        \[
            \begin{aligned}
                \frac{\partial}{\partial \eta} p(y;\eta) &= b(y)\exp(\eta y - \alpha(\eta)) (y - \frac{\partial}{\partial \eta}\alpha(\eta)) = p(y; \eta) (y - \frac{\partial}{\partial \eta}\alpha(\eta)) \\
                \frac{\partial^2}{\partial \eta^2} p(y;\eta) &= p(y;\eta){(y-\frac{\partial}{\partial \eta}\alpha(\eta))}^2 - p(y;\eta) \frac{\partial^2}{\partial \eta^2} \alpha(\eta)
            \end{aligned}
        \]
        then we can further simplify the second-order derivative of the loss function by some algebraic manipulation, and the result is:
        \[
            \frac{\partial^2}{\partial \theta_i \theta_j} l(\theta) = x_i x_j \Var[Y|X;\theta]
        \]
        consider the quadratic form of the hessian matrix, we have:
        \[
            z^T H z = \Var[Y|X; \theta]\sum_i \sum_j x_i x_j z_i z_j = \Var[Y|X;\theta] (z^T x) \geq 0, \forall z \in \mathbb{R}^n
        \]
        
    \end{enumerate}
\end{homeworkProblem}

\pagebreak

\begin{homeworkProblem}
    \begin{enumerate}
        \item
        \begin{enumerate}
            \item Let
            \[
                W = 1/2 \begin{bmatrix}
                    w^{(1)} & 0 & \cdots & 0 \\
                    0 & w^{(2)} & \cdots & 0 \\
                    \vdots & \vdots & \ddots & \vdots \\
                    0 & 0 & \cdots & w^{(n)}
                \end{bmatrix}
            \]
            then we have: \( J(\theta) = {(X\theta - y)}^T W {(X\theta - y)} \).
            \item Let \( \frac{\partial}{\partial \theta} J(\theta) = 0 \), which is equivalent to:
            \[
                \frac{\partial}{\partial \theta} {(X\theta - y)}^T W (X\theta - y) = \frac{\partial}{\partial A} A^T W A \circ \frac{\partial}{\partial \theta} (X\theta - y) \quad (\text{where } A = X\theta - y)
            \]
            since we know: 
            \[ 
                \begin{split}
                &X(\theta + E) - y - (X\theta - y) = XE  \\
                &{(A+E)}^T W (A+E) - A^T W A = A^T W E + E^T W A + E^T W E
                \end{split}
            \]
            we can calculate that the differential of \( J(\theta) \) is (Given input \( E \)):
            \[
                A^T W XE + E^T X^T W A
            \]
            let it be zero, we have:
            \[
                \begin{split}
                & A^T W X E + E^T X^T W A = 0, \forall E \\
                \Leftrightarrow &X^T W^T A = X^T W^T (X\theta - y) = 0  \\
                \Leftrightarrow &X^T W^T X \theta = X^T W^T y   \\
                \Leftrightarrow &\theta = {(X^T W^T X)}^{-1} X^T W^T y
                \end{split}
            \]
        \item Let 
                \[ 
                    \begin{split}
                    J(\theta) &= \prod_{i=1}^m  \frac{1}{\sqrt{2\pi} \sigma^{(i)}} \exp(-\frac{{(y^{(i)} - \theta^T x^{(i)})}^2}{2{(\sigma^{(i)})}^2})
                    \end{split}
                \]
                define \( l(\theta) = \log J(\theta) \), maximize \( l(\theta) \) is equivalent to maximize \( J(\theta) \), \( l(\theta) \) can be wriiten as:
                \[
                    l(\theta) = \log J(\theta) = -\frac{m}{2} \log 2\pi - \sum_{i=1}^m \log \sigma^{(i)} - \sum_{i=1}^m \frac{{(y^{(i)} - \theta^T x^{(i)})}^2}{2{(\sigma^{(i)})}^2}
                \]
                and we find that maximize \( l(\theta) \) is equivalent to:
                \[
                    \text{maximize } \frac{1}{2} \sum_{i=1}^m -\frac{1}{{(\sigma^{(i)})}^2} {(y^{(i)} - \theta^T x^{(i)})}^2
                \]
                which is equivalent to solve the locally weighted linear regression with \( w_i = -\frac{1}{{(\sigma^{(i)})}^2} \).
        \end{enumerate}
        \item Codes are shown in src directory, see \texttt{src/p05b\_lwr.py}.
        \item Codes are shown in src directory, see \texttt{src/p05c\_tau.py}, figure is shown in \texttt{src/output/p05c\_lwr\_tau0.05\_test.png}. \( \tau = 0.05 \) achieves the lowest MSE on the
        \texttt{valid} split, and the MSE on the \texttt{test} split is \texttt{0.012400076150475756} with \( \tau = 0.05 \).
    \end{enumerate}
\end{homeworkProblem}

\end{document}